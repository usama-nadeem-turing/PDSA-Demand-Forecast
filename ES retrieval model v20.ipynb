{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic setup\n",
    "  \n",
    "- Imports\n",
    "- Basic skill map\n",
    "- Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import ast\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "tqdm.pandas()\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "import config\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_challenges():\n",
    "    \"\"\"\n",
    "    returns a dict mapping skills to challenges\n",
    "    \"\"\"\n",
    "    skill_det = pd.io.gbq.read_gbq(\n",
    "    f\"\"\"\n",
    "\n",
    "        SELECT\n",
    "            mjs.skill_id,\n",
    "            bas.skill_name,\n",
    "            ARRAY_AGG(DISTINCT msc.challenge_id IGNORE NULLS) AS challenge_ids,\n",
    "        FROM\n",
    "            (\n",
    "            SELECT DISTINCT skill_id FROM `turing-230020.raw.tpm_developer_skill` \n",
    "            UNION DISTINCT \n",
    "            SELECT DISTINCT skill_id FROM `turing-230020.raw.ms2_job_skill` \n",
    "            )mjs\n",
    "        LEFT JOIN\n",
    "            `raw.ms2_skill_challenges` msc\n",
    "        ON\n",
    "            mjs.skill_id = msc.skill_id\n",
    "        LEFT JOIN\n",
    "            `raw.base_all_skills_v4` bas\n",
    "        ON\n",
    "            mjs.skill_id = bas.id\n",
    "        GROUP BY\n",
    "        1,2\n",
    "\n",
    "\n",
    "    \"\"\",project_id='turing-230020')\n",
    "    \n",
    "    challenge_map = {}\n",
    "    name_map = {}\n",
    "    for skill in skill_det.skill_id.values:\n",
    "        challenge_map[skill] = skill_det[skill_det.skill_id == skill].challenge_ids.values[0]\n",
    "        name_map[skill] = skill_det[skill_det.skill_id == skill].skill_name.values[0]\n",
    "\n",
    "    return challenge_map, name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_map, name_map = get_challenges()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Jobs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills of Active Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must have skills as a list of lists\n",
    "job_skills = pd.io.gbq.read_gbq(\n",
    "f\"\"\"\n",
    "\n",
    "\n",
    "WITH forcasted_tuples AS \n",
    "(  \n",
    "  SELECT \n",
    "    job_id, skill_name, id AS skill_id\n",
    "  FROM\n",
    "  (\n",
    "    SELECT \n",
    "    job_id, \n",
    "    CASE \n",
    "      WHEN (skill_name = ' Laravel') THEN 'Laravel'\n",
    "      WHEN (skill_name = 'Android: Kotlin') THEN 'Android/Kotlin'\n",
    "      ELSE skill_name\n",
    "    END AS skill_name\n",
    "    FROM `turing-230020.product_ds_supply.forecasted_tuples`, UNNEST(SPLIT(Skill_Tuple, '|'))skill_name \n",
    "  ) LEFT JOIN `turing-230020.raw.base_all_skills_v4` USING(skill_name)\n",
    ")\n",
    "\n",
    "\n",
    "  SELECT\n",
    "    job_id,\n",
    "    `matchingmetrics`.structurize_skills(TO_JSON_STRING(ARRAY_AGG(DISTINCT skill_id\n",
    "        ORDER BY\n",
    "          skill_id))) AS skills\n",
    "  FROM\n",
    "    forcasted_tuples\n",
    "  GROUP BY\n",
    "    job_id \n",
    "\"\"\"\n",
    ",project_id='turing-230020')\n",
    "#  jsi.job_id in {tuple(active_jobs.job_id.unique())}\n",
    "job_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_id_dict = {\n",
    "      108: [108, 97, 448]\n",
    "    , 567: [567, 2068]\n",
    "    , 568: [568, 277]\n",
    "    , 65: [65, 221]\n",
    "    , 71: [1157, 71]\n",
    "    , 257: [257, 443, 444, 467, 762, 189, 2140]\n",
    "    , 70: [70, 60]\n",
    "    , 351: [351, 394, 2084]\n",
    "    , 541: [541, 204]\n",
    "    , 1547: [1547, 460, 2114]\n",
    "    , 1598: [1598, 483, 1571, 41]\n",
    "    , 686: [686, 348, 347]\n",
    "    , 25: [25, 2050]\n",
    "    , 554: [554, 555]\n",
    "    , 2096: [2096, 26]\n",
    "    , 387: [387, 308]\n",
    "    , 264: [264, 1408]\n",
    "    , 358: [358, 1928, 2087]\n",
    "    , 1821: [1821, 2061]\n",
    "    , 114: [114, 86, 113, 2097]\n",
    "    , 1938: [1696, 1123, 2150, 614]\n",
    "    , 1025: [1025, 1389]\n",
    "    , 127: [127, 162, 327]\n",
    "    , 301: [301, 1991]\n",
    "    , 1315: [1315, 464, 1675, 1676, 1258]\n",
    "    , 2: [2, 3]\n",
    "    , 2020: [2020, 401, 680]\n",
    "    , 400: [400, 2032]\n",
    "    , 223: [223, 1647]\n",
    "    , 20: [20, 710, 1466]\n",
    "    , 174: [174, 128]\n",
    "    , 258: [258, 73, 1250]\n",
    "    , 1286: [1286, 33, 1063, 812]\n",
    "    , 1420: [1420, 1470]\n",
    "    , 433: [433, 2133, 1827]\n",
    "    , 449: [449, 1465]\n",
    "    , 29: [29, 1688]\n",
    "    , 166: [166, 173]\n",
    "    , 107: [107, 309]\n",
    "    , 125: [125, 1615, 111, 1059, 946, 328]\n",
    "    , 2055: [2055, 425]\n",
    "    , 93: [93, 598]\n",
    "    , 2094: [2094, 256, 678, 2091, 2092, 1349, 1964, 397, 1300]\n",
    "    , 2036: [2036, 1873, 1397, 2154, 1855]\n",
    "    , 1939: [1939, 706, 1822, 153, 707, 1830]\n",
    "}\n",
    "\n",
    "job_skills['original_skills'] = job_skills['skills'] \n",
    "\n",
    "skills = []\n",
    "for _, row in job_skills.iterrows():\n",
    "    job_skill = []\n",
    "    for skill in row['skills'].split(','):\n",
    "        if skill.strip('][').split(', ')[0] != 'null':\n",
    "            temp = int(skill.strip('][').split(', ')[0])\n",
    "            if temp in skill_id_dict.keys():\n",
    "                job_skill.append(skill_id_dict[temp])\n",
    "            else:\n",
    "                job_skill.append([temp])\n",
    "    skills.append(job_skill)\n",
    "\n",
    "job_skills['skills'] = skills\n",
    "\n",
    "job_skills['skills'] = job_skills['skills'].astype(str)\n",
    "job_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_skills.skills = job_skills.skills.map(ast.literal_eval)\n",
    "job_skills['num_skills'] = job_skills.skills.map(len)\n",
    "job_skills['g_skills'] = job_skills.skills.apply(lambda r: [[{\"skillId\":s,'keyword':name_map[s]} for s in skills] for skills in sorted(r)])\n",
    "job_skills['keywords'] = job_skills.skills.apply(lambda r: [name_map[s] for skill in r for s in skill])\n",
    "job_skills['skill_comb'] = job_skills.g_skills.apply(lambda r: str(r))\n",
    "\n",
    "job_skills_dict = {job['job_id']:job['skills'] for id,job in job_skills.iterrows()}\n",
    "job_skills\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Must-Have Skill Retrieval\n",
    "\n",
    "Replication of DE pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.monitor_interval = 0\n",
    "resp = \"\"\n",
    "\n",
    "results = pd.DataFrame({'job_id':[],'dev_id':[],'v20_score':[], 'v20_post_processing_score': []})\n",
    "resp = ''\n",
    "for id,job in tqdm(job_skills.iterrows(),total=len(job_skills)):\n",
    "    with requests.Session() as s:\n",
    "        payload = json.load(open('v20_api_payload_current_retrieval.json', 'r'))\n",
    "\n",
    "        payload['skillKeywordSearch']['mustHave'] = job['g_skills']\n",
    "\n",
    "        payload['searchAnywhereParameters']['mustHaveWords'] = job['keywords']\n",
    "        payload['skillKeywordSearch']['mustHave'] = job['g_skills']\n",
    "        payload['jobId'] = job['job_id']#.astype(str)\n",
    "\n",
    "\n",
    "        json_payload = json.dumps(payload)\n",
    "        content_length = str(len(json_payload))\n",
    "\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Content-Length': content_length,\n",
    "            'authorization': config.bearer_token\n",
    "        }\n",
    "\n",
    "        resp = s.post(config.url, json_payload, headers = headers)\n",
    "        try:\n",
    "\n",
    "            df1 = pd.DataFrame(pd.DataFrame(json.loads(json.dumps(resp.json()['debug']['feature_df']))).loc[\"modelScore\"]).reset_index()\n",
    "            df1.rename(columns = {'modelScore' : 'v20_score', 'index':'dev_id'}, inplace = True)\n",
    "            df1['dev_id'] = df1['dev_id'].astype(int)\n",
    "            df1['v20_score'] = df1['v20_score'].astype(float)\n",
    "\n",
    "            df2 = pd.DataFrame(json.loads(json.dumps(resp.json()['developers'])))\n",
    "            df2.rename(columns = {'score' : 'v20_post_processing_score', 'userId':'dev_id'}, inplace = True)\n",
    "            df2['dev_id'] = df2['dev_id'].astype(int)\n",
    "            df2['v20_post_processing_score'] = df2['v20_post_processing_score'].astype(float)\n",
    "\n",
    "            resp_df = df1.merge( df2,on=['dev_id'],how='left')\n",
    "\n",
    "            resp_df['job_id'] = job['job_id']\n",
    "            results = results.append(resp_df)\n",
    "            print(f\"job: {job['job_id']} : Done\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"job: {job['job_id']} : Error - {resp.status_code}: {resp.text}\")\n",
    "            continue\n",
    "results.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge conditions, add miscellaneous conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Extraction_Date'] = time.strftime(\"%Y-%m-%d\", time.localtime())\n",
    "results_f = results[['Extraction_Date', 'job_id','dev_id', 'v20_score', 'v20_post_processing_score']]\n",
    "\n",
    "results_f['job_id'] = results_f['job_id'].astype(int)\n",
    "results_f['dev_id'] = results_f['dev_id'].astype(int)\n",
    "results_f['v20_score'] = results_f['v20_score'].astype(float)\n",
    "results_f['v20_post_processing_score'] = results_f['v20_post_processing_score'].astype(float)\n",
    "results_f\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "# Replace 'your_project_id', 'your_dataset_id', and 'your_table_id' with your actual values\n",
    "project_id = 'turing-230020'\n",
    "dataset_id = 'product_ds_supply'\n",
    "table_id = 'forecasted_tuples_supply_v20'\n",
    "\n",
    "# Insert the DataFrame into the BigQuery table\n",
    "\n",
    "pandas_gbq.to_gbq(results_f, f'{dataset_id}.{table_id}', project_id=project_id, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01f29a38e4d22e4c46d74f7bef387ce4e9481c41b93ea1b5baefe6178a26e547"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
